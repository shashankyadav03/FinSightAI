{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb14d27-d3c7-4db5-977c-ed1f740d54c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9baec7ae-0325-4523-9e06-79cd47ac5bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dc89e832d349e39223dc5c3e765d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 40.0M/40.0M [00:03<00:00, 10.1MB/s]\n",
      "Downloading data: 100%|██████████| 1.47M/1.47M [00:00<00:00, 2.65MB/s]\n",
      "Downloading data: 100%|██████████| 2.52M/2.52M [00:00<00:00, 4.37MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2794c126785642008c6e4ae051039ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7e1232378f47ad9e5d7521406b1985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52dfb3488ac4f23a67c540c4a1dddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load and prepare the dataset\n",
    "dataset = load_dataset(\"raeidsaqur/nifty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54091e77-dfab-4f94-ad23-0238b0c2ba0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Load the pre-trained FinGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb14449b-772c-4a03-8651-4348015a2a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4258d1-8841-47ce-81e0-c0678dc4c670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LoraModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Wrap the model with LoRA\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m \u001b[43mLoraModel\u001b[49m(model)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      8\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finetuned_fingpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LoraModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "lora_model = LoraModel(model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_fingpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef023f3e-be54-4deb-b11b-4044c4786a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./finetuned_fingpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7365d32b-940d-4c08-85b4-341f03933c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoraModel' from 'lora' (/home/jovyan/FinSightAI-Shashank/Virtual/myenv/lib/python3.10/site-packages/lora/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LoraModel' from 'lora' (/home/jovyan/FinSightAI-Shashank/Virtual/myenv/lib/python3.10/site-packages/lora/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./finetuned_fingpt\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./finetuned_fingpt\")\n",
    "\n",
    "# Inference pipeline\n",
    "from transformers import pipeline\n",
    "nlp_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate financial insights\n",
    "text = \"The market trend for the upcoming quarter is\"\n",
    "generated_text = nlp_pipeline(text, max_length=100, num_return_sequences=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc6269-79bf-4bba-87fe-71337027855d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
